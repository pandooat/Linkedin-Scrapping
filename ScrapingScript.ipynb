{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37966c75",
   "metadata": {},
   "source": [
    "# Web Scraping Linkedin Job Listings with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f6fea0",
   "metadata": {},
   "source": [
    "Reference : https://maoviola.medium.com/a-complete-guide-to-web-scraping-linkedin-job-postings-ad290fcaa97f\n",
    "\n",
    "Script ini adalah versi yang diperbarui untuk tampilan LinkedIn terkini per April 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf14e8",
   "metadata": {},
   "source": [
    "   \"Pre-requisite:,\n",
    "* Have python > 3.0 installed : https://www.python.org/downloads/windows/\\n\",\n",
    "* Ensure pip or anaconda is installed \\n\",\n",
    "* Have jupyter notebook installed : https://jupyter.org/install (if using pip) or https://anaconda.org/anaconda/jupyter (if using anaconda)\\n\",\n",
    "* Have Selenium WebDriver installed : https://pypi.org/project/selenium/ (if using pip) or https://anaconda.org/conda-forge/selenium (if using anaconda)\\n\",\n",
    "* Have Pandas installed\\n\",\n",
    "* Download chrome webdriver : https://chromedriver.chromium.org/downloads (make sure it supports your Chrome version!)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a88351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0f044",
   "metadata": {},
   "source": [
    "## 1. Opening browser & scroll the job listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define browser and action setup\n",
    "PATH = \"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(PATH)\n",
    "\n",
    "# Define URL\n",
    "linkedin_url = \"https://www.linkedin.com/jobs/search/?keywords=data%20scientist&location=Indonesia\"\n",
    "\n",
    "# Action Steps\n",
    "driver.maximize_window()\n",
    "driver.get(linkedin_url) # Open web page\n",
    "\n",
    "# Determine how many jobs we want to scrape, and calculate how many time we need to scroll down\n",
    "no_of_jobs = 1500\n",
    "# int(driver.find_element_by_css_selector(\"h1>span\").get_attribute(\"innerText\"))\n",
    "n_scroll = int(no_of_jobs/25)+1\n",
    "print(n_scroll)\n",
    "i = 1\n",
    "driver.execute_script(\"return document.body.scrollHeight\") #scroll to top\n",
    "while i <= n_scroll:\n",
    "    driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    time.sleep(2) # wair for 2 seconds\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") #scroll to the bottom of page\n",
    "    time.sleep(3)\n",
    "    i = i + 1\n",
    "    try:\n",
    "        button = driver.find_element(By.XPATH,\"/html/body/div[1]/div/main/section[2]/button\")\n",
    "        #time.sleep(2)\n",
    "        button.click()\n",
    "        time.sleep(1)\n",
    "        print(\"load more click\")\n",
    "    except:\n",
    "        driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        time.sleep(3)\n",
    "\n",
    "print (\"total jobs :\")\n",
    "jobs = driver.find_element(By.CLASS_NAME,\"jobs-search__results-list\").find_elements(By.TAG_NAME,'li') # return a list\n",
    "print(len(jobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba1252",
   "metadata": {},
   "source": [
    "## 2. Get Main Attributes of each Job Listing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3587a",
   "metadata": {},
   "source": [
    "Important notes : \n",
    "* The HTML and CSS element path needs to be checked regularly, because it's possible that it will change in the future\n",
    "* You can also group all the possibly changing web elements in one place\n",
    "* This is not the most efficient code ever. It works but definitely needs improvement, and feel free to do so on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a4da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id= []\n",
    "job_title = []\n",
    "company_name = []\n",
    "location = []\n",
    "date = []\n",
    "job_link = []\n",
    "for job in jobs:\n",
    "    job_id0 = job.get_attribute('data-entity-urn')\n",
    "    job_id.append(job_id0)\n",
    "\n",
    "    job_title0 = job.find_element(By.CSS_SELECTOR,'h3').get_attribute('innerText')\n",
    "    job_title.append(job_title0)\n",
    "\n",
    "    company_name0 = job.find_element(By.CSS_SELECTOR,'h4').get_attribute('innerText')\n",
    "    company_name.append(company_name0)\n",
    "\n",
    "    location0 = job.find_element(By.CLASS_NAME,'job-search-card__location').get_attribute('innerText')\n",
    "    location.append(location0)\n",
    "\n",
    "    date0 = job.find_element(By.CSS_SELECTOR,'div>div>time').get_attribute('datetime')\n",
    "    date.append(date0)\n",
    "\n",
    "    job_link0 = job.find_element(By.CSS_SELECTOR,'a').get_attribute('href')\n",
    "    job_link.append(job_link0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a66241",
   "metadata": {},
   "outputs": [],
   "source": [
    "jd = []\n",
    "seniority = []\n",
    "emp_type = []\n",
    "job_func = []\n",
    "industries = []\n",
    "for item in range(len(jobs)):\n",
    "    job_func0=[]\n",
    "    industries0=[]\n",
    "    # clicking job to view job details\n",
    "    try:\n",
    "        job_click_path = f'/html/body/div[1]/div/main/section[2]/ul/li[{item+1}]/a/div[1]/img'\n",
    "        job_click = job.find_element(By.XPATH,job_click_path).click()\n",
    "    except:\n",
    "        job_click_path = f'/html/body/div[1]/div/main/section[2]/ul/li[{item+1}]/div/a'\n",
    "    \n",
    "    print(job_click_path)\n",
    "    job_click = job.find_element(By.XPATH,job_click_path).click()\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        jd_path = '/html/body/div[1]/div/section/div[2]/div/section[1]/div/div/section'\n",
    "        jd0 = job.find_element(By.XPATH,jd_path).get_attribute('innerText')\n",
    "    except:\n",
    "        jd_path = '/html/body/div[1]/div/section/div[2]/div/section[2]/div/div/section/div'\n",
    "    \n",
    "    jd0 = job.find_element(By.XPATH,jd_path).get_attribute('innerText')\n",
    "    is_benefit = True\n",
    "    try:\n",
    "        assert 'Base pay range' in jd0\n",
    "    except:\n",
    "        is_benefit = False\n",
    "        \n",
    "    if is_benefit==True :\n",
    "        jd_path = '/html/body/div[1]/div/section/div[2]/div/section[2]/div/div/section/div'\n",
    "        jd0 = job.find_element(By.XPATH,jd_path).get_attribute('innerText')\n",
    "        jd.append(jd0)\n",
    "#         print(jd_path)\n",
    "#         print(\"appended\")\n",
    "        jd_path2 = '/html/body/div[1]/div/section/div[2]/div/section[2]/div'\n",
    "    else:\n",
    "#         print(jd_path)\n",
    "        jd.append(jd0)\n",
    "#         print(\"appended\")\n",
    "        jd_path2 = '/html/body/div[1]/div/section/div[2]/div/section[1]/div'\n",
    "\n",
    "    try:\n",
    "        seniority_path = jd_path2 + '/ul/li[1]/span'\n",
    "        seniority_class_name = 'description__job-criteria-text description__job-criteria-text--criteria'\n",
    "        #seniority0 = job.find_element(By.XPATH,seniority_path).get_attribute('innerText')\n",
    "        seniority0 = job.find_element(By.CLASS_NAME,seniority_class_name).get_attribute('innerText')\n",
    "        seniority.append(seniority0)\n",
    "    except:\n",
    "        seniority.append('') #handling if seniority is not available\n",
    "    \n",
    "    try:\n",
    "        emp_type_path = jd_path2 + '/ul/li[2]/span'\n",
    "        emp_type_class_name = ''\n",
    "        #emp_type0 = job.find_element(By.XPATH,emp_type_path).get_attribute('innerText')\n",
    "        emp_type0 = job.find_element(By.CLASS_NAME,emp_type_class_name).get_attribute('innerText')\n",
    "        emp_type.append(emp_type0)\n",
    "    except:\n",
    "        emp_type.append('') #handling if employment type is not available\n",
    "    \n",
    "    try:\n",
    "        job_func_path = jd_path2 + '/ul/li[3]/span'\n",
    "        job_func_class_name = ''\n",
    "        #job_func0 = job.find_element(By.XPATH,job_func_path).get_attribute('innerText')\n",
    "        job_func0 = job.find_element(By.CLASS_NAME,job_func_class_name).get_attribute('innerText')\n",
    "        job_func.append(job_func0)\n",
    "    except:\n",
    "        job_func.append('') #handling if job function is not available\n",
    "    \n",
    "    try:\n",
    "        industries_path = jd_path2 + '/ul/li[4]/span'\n",
    "        industries_class_name = ''\n",
    "        #industries0 = job.find_element(By.XPATH,industries_path).get_attribute('innerText')\n",
    "        emp_type0 = job.find_element(By.CLASS_NAME,industries_class_name).get_attribute('innerText')\n",
    "        industries.append(industries0)\n",
    "    except:\n",
    "        industries.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data = pd.DataFrame({'ID': job_id,\n",
    "                        'Date': date,\n",
    "                        'Company': company_name,\n",
    "                        'Title': job_title,\n",
    "                        'Location': location,\n",
    "                        'Description' : jd,\n",
    "                        'Level': seniority,\n",
    "                        'Type': emp_type,\n",
    "                        'Function': job_func,\n",
    "                        'Industry': industries\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5699248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change output file path to your desired path. Don't forget to put the filename and filetype also\n",
    "output_file_path = 'C:/Users/pandooat/Documents/_WORK/Portofolio/linkedinwebscraping/DataScience.xlsx'\n",
    "job_data.to_excel(output_file_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
